{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Decision Trees\n",
        "\n",
        "## Introduction\n",
        "\n",
        "We can quickly use the sklearn library to build and use decision trees. For example,\n",
        "\n",
        "```\n",
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
        "clf = clf.fit(X, y)\n",
        "```\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/78/Petal-sepal.jpg\" width=\"200\" align=\"right\" />\n",
        "\n",
        "However this lab focuses on learning how the decision trees and random forests are built, by writing our own versions of these tools. In the real world you would be using a library, but the aim here is understanding the fundamentals of how these tools work.\n",
        "\n",
        "This week I've included the answers to the exercises at the end of the document. Please still try to work out the answers yourself!\n",
        "\n",
        "We will use as our purity metric the conditional entropy (as covered in the lecture).\n",
        "\n",
        "We will use a discretised (binarised) version of the <a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">iris dataset</a>. I've converted each of the continuous features to a binary feature, e.g. petal length is now 'long petal' (true/false). The labels are still three categories, either 'setosa', 'versicolor', 'virginica'. The task is to classify the species of Iris from four features (petal length, petal width, sepal length and sepal width).\n",
        "\n",
        "I've tried to visualise this a little by plotting a bar graph for each class, with the number of rows for each feature being true. For example none of the 50 rows of the 'setosa' species have long sepals, while 88% of the rows of the virginica speices have long sepals."
      ],
      "metadata": {
        "id": "ZqZnVCd46pP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "X = pd.read_csv('https://drive.usercontent.google.com/download?id=1SHpvd9kee9eiAeviGfOgKR3SdOGfz8yM&export=download&authuser=0',index_col=0)\n",
        "y = pd.read_csv('https://drive.usercontent.google.com/download?id=1SicnjGCccMnOETeAVA98iCq74cc8_OyD&export=download&authuser=0',index_col=0)\n",
        "for i,v in enumerate(y['class'].unique()):\n",
        "  plt.subplot(1,3,i+1)\n",
        "  plt.title(v)\n",
        "  print(X[y['class']==v].mean())\n",
        "  X[y['class']==v].mean().plot.bar()\n",
        "  plt.ylim([0,1])\n",
        "  if i>0: plt.yticks([],[])"
      ],
      "metadata": {
        "id": "r7t6PUxZWX8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entropy\n",
        "\n",
        "We need to compute the entropy first.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6uN58ELCZ1tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.1\n",
        "\n",
        "Fill in the rest of this function with code to compute the entropy of the `ydata` column:"
      ],
      "metadata": {
        "id": "3pFOuZi39voA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_entropy(ydata):\n",
        "  \"\"\"\n",
        "  Returns the entropy of the labels in ydata (a pandas series) in bits.\n",
        "  \"\"\"\n",
        "  #answer here\n",
        "  return #?"
      ],
      "metadata": {
        "id": "AXmZW0hR87A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "pWzovOIc8A0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To test you have coded this correctly, this synthetic test data should lead to\n",
        "#an estimate of entropy of about 1.72957 bits\n",
        "#as there are 6 red, 3 blue, 2 yellow, 1 orange. We can assume their probabilities,\n",
        "#and from that compute the entropy:\n",
        "#-((6/12)*np.log2(6/12) + (3/12)*np.log2(3/12) + (2/12)*np.log2(2/12) + (1/12)*np.log2(1/12))\n",
        "testlabels = pd.DataFrame(['red','blue','blue','blue','red','red','yellow','red','red','red','yellow','orange'])\n",
        "assert abs(compute_entropy(testlabels)-1.72957)<0.0001\n",
        "\n",
        "#check your method computes the entropy correctly! The assertion will raise an\n",
        "#AssertionError if it's wrong!"
      ],
      "metadata": {
        "id": "Yr-Sa6KpbrQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can compute the entropy of the Iris labels:\n",
        "compute_entropy(y)"
      ],
      "metadata": {
        "id": "nDlvkQogeJTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional Entropy\n",
        "\n",
        "In the lecture, I wrote that the conditional entropy was,\n",
        "\n",
        "$$-\\sum\\limits_{x \\in X} \\sum\\limits_{y \\in Y}p(x,y) \\log_2 p(y|x)$$\n",
        "\n",
        "but it will be slightly easier to code if we rewrite it like this:\n",
        "\n",
        "First we use the product rule, $p(x,y) = p(y|x)p(x)$ to write the above as,\n",
        "\n",
        "$$-\\sum\\limits_{x \\in X} \\sum\\limits_{y \\in Y}p(y|x)p(x) \\log_2 p(y|x)$$\n",
        "\n",
        "Then we note that $p(x)$ can move out of the inner summation, as it only depends on $x$:\n",
        "\n",
        "$$-\\sum\\limits_{x \\in X} p(x) \\sum\\limits_{y \\in Y}p(y|x) \\log_2 p(y|x)$$\n",
        "\n",
        "So another way of writing the conditional entropy is \"the expectation, over X, of the entropy of Y, given X\".\n",
        "\n",
        "Sort of like this (if it helps you remember),\n",
        "$$\n",
        "\\mathbb{E}_X \\Big[H(Y|X)\\Big]\n",
        "$$\n",
        "\n",
        "This means you could write you conditional entropy function as:\n",
        "\n",
        "1. loop over all the unique values in Xdata. Hint, try: `for v in Xdata.unique():`\n",
        "2. For each one find the probability of the series having that value `np.mean(Xdata==v)`\n",
        "3. Compute the entropy in ydata for just the rows in which Xdata is that value (`v`). You could do this by calling your `compute_entropy` method, with just the relevant values of ydata: `ydata[Xdata==v]`.\n",
        "4. For each of the above compute the product of the probability you calculated in (2) and the entropy in (3), and sum them all, to get the expected entropy.\n",
        "5. Return this sum.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "a2cRgjrTFwgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.2\n",
        "\n",
        "Write a function to compute the conditional entropy,"
      ],
      "metadata": {
        "id": "jft34t9P9r-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_conditional_entropy(Xdata,ydata):\n",
        "  \"\"\"Return the Conditional entropy of ydata given the series (aka 'column') Xdata\n",
        "\n",
        "   - Xdata, a series (i.e. a single column) from a pandas dataframe, e.g. X['long petal']\n",
        "   - ydata, the associated labels\n",
        "\n",
        "  returns the conditional entropy H(Y|X)\n",
        "  \"\"\"\n",
        "  #answer here\n",
        "  return #?"
      ],
      "metadata": {
        "id": "EU4270OBD-6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "jrK_HC8t8K19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testdata = pd.Series([False,True,True,True,False,False,True,False,False,False,True,True])\n",
        "testlabels = pd.DataFrame(['red','blue','blue','blue','red','red','yellow','red','red','red','yellow','orange'])\n",
        "\n",
        "#this testdata is 'false' when the labels were red, and true otherwise:\n",
        "#so if false the entropy will be zero; if true, the entropy will be:\n",
        "#3 blue, 2 yellow, 1 orange\n",
        "#-(3/6 * np.log2(3/6) + 2/6 * np.log2(2/6) + 1/6 * np.log2(1/6))\n",
        "#1.4591479170272448\n",
        "#so the conditional entropy will be 0.5 * 0 + 0.5 * 1.45915 = 0.72957.\n",
        "\n",
        "assert abs(compute_conditional_entropy(testdata,testlabels)-0.72957)<0.0001\n"
      ],
      "metadata": {
        "id": "279B7VkwaS40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the two methods you have written to compute the mutual information (information gain) for each of the features.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SgEVWBH4b4ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.3\n",
        "\n",
        "Can you finish off this method, what is missing?\n",
        "\n",
        "Hint:\n",
        "- We're computing $H(Y) - H(Y|X)$.\n",
        "- $H(Y)$ is stored in `H_Y`.\n",
        "- $H(Y|X)$ can be computed using `compute_conditional_entropy(Xdata[col],ydata)`."
      ],
      "metadata": {
        "id": "WkCjz5bw-RL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mutual_infos(Xdata,ydata):\n",
        "  \"\"\"\n",
        "  For each column in Xdata, compute the mutual information with ydata\n",
        "  (also known as information gain):\n",
        "\n",
        "     H(Y) - H(Y|X_column)\n",
        "\n",
        "  returns a pandas series with the mutual information for each column.\n",
        "  \"\"\"\n",
        "  mutual_infos = {}\n",
        "  H_Y = compute_entropy(ydata)\n",
        "  for col in Xdata.columns:\n",
        "    mutual_infos[col] = #answer here\n",
        "  return pd.Series(mutual_infos)\n",
        "\n",
        "compute_mutual_infos(X,y)"
      ],
      "metadata": {
        "id": "xpLnM-ipIskl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to build the tree, at each split, we will pick the feature that has the greatest information gain:"
      ],
      "metadata": {
        "id": "hrh8fZ4Eb-ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tree(Xdata,ydata,depth=np.inf):\n",
        "  mutual_infos = compute_mutual_infos(Xdata,ydata)\n",
        "  col = mutual_infos.idxmax()\n",
        "  if (mutual_infos[col]==0) or (depth<=0):\n",
        "    return ydata.value_counts().idxmax()[0]\n",
        "\n",
        "  tree = {'split':col}\n",
        "  for v in Xdata[col].unique():\n",
        "    subtree = build_tree(Xdata[Xdata[col]==v],ydata[Xdata[col]==v],depth-1)\n",
        "    tree[v] = subtree\n",
        "\n",
        "  return tree\n",
        "\n",
        "#just using every other data point to train the tree. Max depth set to 2, try also max depth=1.\n",
        "tree = build_tree(X[::2],y[::2],2)\n",
        "print(tree)"
      ],
      "metadata": {
        "id": "GtJHtyhsSoOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(tree,x):\n",
        "  subtree = tree[x[tree['split']]]\n",
        "  if isinstance(subtree, str):\n",
        "    return subtree\n",
        "  else:\n",
        "    return predict(subtree,x)\n",
        "\n",
        "predictions = []\n",
        "for i,xtest in X[1::2].iterrows():\n",
        "  predictions.append(predict(tree,xtest))\n",
        "\n",
        "print(\"Accuracy: %0.2f%%\" % (100*np.mean(predictions==y[1::2]['class'].values)))"
      ],
      "metadata": {
        "id": "gKxmyNYJf7EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sklearn's decision tree\n",
        "\n",
        "Let's compare this tree with that generated with sklearn."
      ],
      "metadata": {
        "id": "i1HNOWTIeQqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=2)\n",
        "clf = clf.fit(X[::2], y[::2])"
      ],
      "metadata": {
        "id": "JC0SlheQ9NbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see what sort of tree sklearn creates using its 'tree' methods. The plot is somewhat unclear, but one can (after a bit of thinking) see that this matches our own tree:"
      ],
      "metadata": {
        "id": "NMfkDLgZegcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "tree.plot_tree(clf,feature_names=X.columns,class_names=y['class'].unique())"
      ],
      "metadata": {
        "id": "QGbsJYZkJGfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y[1::2], clf.predict(X[1::2]))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
        "disp.plot()"
      ],
      "metadata": {
        "id": "aHFCpM8FBJ9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Bootstrapping / Bagging\n",
        "\n",
        "In the second half of this notebook we will look at applying the bootstrap sampling approach to a simple (one dimensional) regression problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "dLchm_owBqDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrapping applied to simple linear regression\n",
        "\n",
        "We start with just a linear (1st-order \"y = mx+c\") fit to the data. Obviously we could use various library functions, but I'll try to keep the implementation within this notebook so you can see each step."
      ],
      "metadata": {
        "id": "7AqVq8m69jgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#our synthetic dataset\n",
        "xvals = np.array([1.2,1.7,2.4,3.3,4.1,4.8,5.6,7.2,8.4,9.1])\n",
        "yvals = np.array([3.2,3.7,4.2,4.6,4.9,5.1,5.6,5.9,6.1,6.4])\n",
        "\n",
        "def fit_parameters(x,y):\n",
        "  #Ordinary Least Squares fit to a 1st order polynomial\n",
        "  # (straight-line).\n",
        "  #Returns the two parameters as a numpy array\n",
        "  # [gradient, offset]\n",
        "  X = np.c_[x,np.ones_like(x)]\n",
        "  return np.linalg.inv(X.T@X) @ X.T @ y\n",
        "\n",
        "w = fit_parameters(xvals,yvals)\n",
        "plt.plot(xvals,yvals,'x')\n",
        "plt.plot([0,10],w[0]*np.array([0,10])+w[1])"
      ],
      "metadata": {
        "id": "cAVCsh9Bpucq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Exercise 2.1\n",
        "\n",
        "We want to find out the uncertainty (given this model) on the range of possible predictions. To do this we will use bootstrapping and aggregation (bagging).\n",
        "\n",
        "Complete the function below, you will need to:\n",
        "1. Loop 1000 times...\n",
        "2. For each iteration, select randomly *with replacement* ten of the values in x and y. Tip `np.random.choice(len(x),len(x))` might be useful!\n",
        "3. For this selection compute $w$ using the `fit_parameters` method.\n",
        "4. Add $w$ as another row in a matrix recording all the $w$s computed.\n",
        "5. At the end of the loop there should be a $1000 \\times 2$ matrix of values of $w$. Return this matrix."
      ],
      "metadata": {
        "id": "xGdMBHHXuWp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap(x,y):\n",
        "  #answer here\n",
        "  return #?"
      ],
      "metadata": {
        "id": "bPqK5wUHpMdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "It can be informative to plot the distribution over $w$. This gives a sense of what sort of distribution it might be, etc."
      ],
      "metadata": {
        "id": "0bXEcoCAvZsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = bootstrap(xvals,yvals)\n",
        "plt.scatter(ws[:,0],ws[:,1],1)\n",
        "plt.xlabel('Gradient')\n",
        "plt.ylabel('Offset')"
      ],
      "metadata": {
        "id": "ebeB5FX4p05X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot the predictions on our normal xy graph. I'll just plot out 10% of them so the graph isn't overwhelmed. I've also drawn in the 95% credible interval for the predictions."
      ],
      "metadata": {
        "id": "cScBN7yevjWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotCIs(x,y,ws):\n",
        "  testx = np.linspace(0,10,100)\n",
        "\n",
        "  #I generate predictions for a grid of points, to allow us to draw the CIs.\n",
        "  preds = ws[:,0:1]*testx[None,:]+ws[:,1:2]\n",
        "  preds = np.sort(preds,0)\n",
        "  plt.plot(testx,preds[25,:],'b--')\n",
        "  plt.plot(testx,preds[500,:],'b-')\n",
        "  plt.plot(testx,preds[975,:],'b--')\n",
        "\n",
        "  #plot the raw data\n",
        "  plt.plot(x,y,'xr')\n",
        "\n",
        "  #plot 10% of the predictions\n",
        "  for w in ws[::10]:\n",
        "    plt.plot([0,10],[w[1],w[1]+w[0]*10],'k-',alpha=0.05)\n",
        "plotCIs(xvals,yvals,ws)"
      ],
      "metadata": {
        "id": "w7m6xacEx7by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision trees and Bagging\n",
        "\n",
        "Bagging helps when applied to the output of models or algorithms that are \"unstable\", e.g. decision trees, neural networks: These have high variance, but low bias. Linear regression on the other hand has fairly low variance (look how close the different results are above -- it is less likely to be sensitive to resampling).\n",
        "\n",
        "We will now use bagging with simple 1-deep decision trees."
      ],
      "metadata": {
        "id": "C5SP52pMzgWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision trees for a continuous input.\n",
        "\n",
        "We have largely concerned ourselves with binary features. But if you remember in the lecture we briefly touched on continuous inputs and continuous outputs. The purity metric for continuous outputs is the expected variance; and to handle the input one selects a threshold for that feature that minimises the expected variances. The code below implements this:"
      ],
      "metadata": {
        "id": "9yINpHbS9fxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_split(x,y):\n",
        "  \"\"\"This method finds the threshold value that minimises the weighted\n",
        "  sum of variances between the two sub-nodes. It returns this threshold\n",
        "  and the means of the two sub-nodes.\n",
        "  \"\"\"\n",
        "\n",
        "  minimum_var = np.inf\n",
        "  minimum_threshold = np.NaN\n",
        "  for threshold in np.linspace(0,10,100):\n",
        "    weighted_sum_of_variance = np.var(y[y<threshold])*np.sum(y<threshold)+np.var(y[y>=threshold])*np.sum(y>=threshold)\n",
        "    if weighted_sum_of_variance<minimum_var:\n",
        "      minimum_var = weighted_sum_of_variance\n",
        "      minimum_threshold = threshold\n",
        "  return minimum_threshold, np.mean(y[x<minimum_threshold]), np.mean(y[x>=minimum_threshold])"
      ],
      "metadata": {
        "id": "bPt0PsofqCBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param  = get_split(xvals,yvals)\n",
        "print(param)\n",
        "plt.plot([0,param[0],param[0],10],[param[1],param[1],param[2],param[2]],'k-')\n",
        "plt.plot(xvals,yvals,'x')"
      ],
      "metadata": {
        "id": "rePna29Z013B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two things to note:\n",
        "1. The one-deep decision tree doesn't really seem to do a great job at prediction in this case.\n",
        "2. It is probably going to be quite sensitive to resampling.\n",
        "\n",
        "### Exercise 2.2.\n",
        "\n",
        "Try writing a method that resamples 1000 times, with replacement, the points, and then for each resampled set, computes a new split (using the `get_split` method). Put all the parameters into a $1000 \\times 3$ array. It is 3 wide as it includes the value of x where the tree splits the data, and includes the values on either side (left and right). Save in [splitLocation, leftValue, rightValue] order."
      ],
      "metadata": {
        "id": "MfqxhWww10jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_trees(x,y):\n",
        "  \"\"\"Run the bootstrap algorithm to make splits on bootstrap subsets of the data,\n",
        "  then fit the 1-deep decision tree, using the get_split method. Return a numpy\n",
        "  array of 1000x3 values, with the first column equal to the split points, the\n",
        "  second column by the mean on the left of the split, and the third column the\n",
        "  mean of the right of the split.\"\"\"\n",
        "\n",
        "  #answer here\n",
        "  return #?"
      ],
      "metadata": {
        "id": "_JwuUutdqLhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = bootstrap_trees(xvals,yvals)"
      ],
      "metadata": {
        "id": "cwoSdLRyqPPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot these predictions\n",
        "for param in parameters:\n",
        "  plt.plot([0,param[0],param[0],10],[param[1],param[1],param[2],param[2]],'k-',alpha=0.02)\n",
        "\n",
        "#here we plot the 95% CI:\n",
        "pred_x = np.linspace(0,10,100)\n",
        "v = [np.nanmean(np.r_[parameters[p<parameters[:,0],1],parameters[p>=parameters[:,0],2]]) for p in pred_x]\n",
        "lowerCI = [np.nanpercentile(np.r_[parameters[p<parameters[:,0],1],parameters[p>=parameters[:,0],2]],2.5) for p in pred_x]\n",
        "upperCI = [np.nanpercentile(np.r_[parameters[p<parameters[:,0],1],parameters[p>=parameters[:,0],2]],97.5) for p in pred_x]\n",
        "plt.plot(pred_x,v,'b-')\n",
        "plt.plot(pred_x,lowerCI,'b--')\n",
        "plt.plot(pred_x,upperCI,'b--')\n",
        "plt.plot(xvals,yvals,'xr')"
      ],
      "metadata": {
        "id": "NUUG_GpHqSNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Argubly linear regression was a better model choice; however the mean of the ensemble is better than the single decision tree (feel free to check this!) and the model also provides uncertainty estimatation compared to a single tree."
      ],
      "metadata": {
        "id": "DpF9ARR63TIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Random Forest\n",
        "\n",
        "We finally will apply what we have learnt to build a random forest.\n",
        "\n",
        "This dataset is already very well separated with a simple decision tree, so a random forest is unlikely to help. We include the approach here purely for demonstrating how random forest works. Random forest is more effective where there is a greater risk of overfitting, and/or where many more features are available.\n"
      ],
      "metadata": {
        "id": "S0ieslQGcN2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative tree-building method with subspace sampling\n",
        "\n",
        "This method creates a tree as before, but at each node we only consider a random subset of the features. You can change the number of features chosen by changing `n=2` to, e.g. `n=1` in this line: `mutual_infos.sample(n=2)`."
      ],
      "metadata": {
        "id": "byDM-P6Fo4nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tree_with_subspace_sampling(Xdata,ydata,depth=np.inf):\n",
        "  \"\"\"\n",
        "  This method builds a decision tree, but at each node, we only keep 2 of the\n",
        "  four features. This is called \"subspace sampling\".\n",
        "  \"\"\"\n",
        "  mutual_infos = compute_mutual_infos(Xdata,ydata)\n",
        "  mutual_infos = mutual_infos.sample(n=2) #just keep two of the four features.\n",
        "  col = mutual_infos.idxmax()\n",
        "  if (mutual_infos[col]==0) or (depth<=0):\n",
        "    return ydata.value_counts().idxmax()[0]\n",
        "\n",
        "  tree = {'split':col}\n",
        "  for v in Xdata[col].unique():\n",
        "    subtree = build_tree(Xdata[Xdata[col]==v],ydata[Xdata[col]==v],depth-1)\n",
        "    tree[v] = subtree\n",
        "\n",
        "  return tree\n",
        "\n",
        "#just using every other data point to train the tree. Max depth set to 2.\n",
        "build_tree_with_subspace_sampling(X[::2],y[::2],2)"
      ],
      "metadata": {
        "id": "dJ0VGFqwdldh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the random forest\n",
        "\n",
        "Let's now build our forest. Make lots of trees using the subspace sampling. Here we make 50 different trees, each one might be different from the others as at each branch of the tree, we could be using a different subset of features. A given, individual tree, will therefore probably be worse on the test data than the standard decision tee created above, however by making them have less correlation with each other, we are able to reduce our overall error on the test set. This seems somewhat counter-intuitive!"
      ],
      "metadata": {
        "id": "7v8GLkQymdmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create 50 trees by calling build_tree_with_subspace_sampling\n",
        "trees = []\n",
        "for i in range(50):\n",
        "  trees.append(build_tree_with_subspace_sampling(X[::2],y[::2],2))\n",
        "\n",
        "#make predictions on each test item (the even items in the datasets)\n",
        "predictions = []\n",
        "for i,xtest in X[1::2].iterrows():\n",
        "  singletree_predictions = []\n",
        "  for tree in trees: #we record the predictions from all the individual trees...\n",
        "    singletree_predictions.append(predict(tree,xtest))\n",
        "  #we get the most popular prediction from all the trees in the random forest...\n",
        "  predictions.append(max(set(singletree_predictions), key=singletree_predictions.count))"
      ],
      "metadata": {
        "id": "atRZc0bij78I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now compute the accuracy by looking at how many of our predictions matched the true label."
      ],
      "metadata": {
        "id": "NTiv7v2qqMfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: %0.2f%%\" % (100*np.mean(predictions==y[1::2]['class'].values)))"
      ],
      "metadata": {
        "id": "z9FmO2Axkc76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To explore the random forest approach a little more,\n",
        "- set the max-depth to 1\n",
        "- use 500 trees (to reduce the variability)\n",
        "- reduce the subspace sampling to just pick one feature instead of two.\n",
        "\n",
        "How does the accuracy compare to the single decision tree (**with depth=1**)? Does the random forest do better than the single tree? (I've not included the answer below, maybe discuss with friends!).\n"
      ],
      "metadata": {
        "id": "hcG-lxvwmns3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answers\n"
      ],
      "metadata": {
        "id": "BOVo2j4L7otk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.1"
      ],
      "metadata": {
        "id": "GiuLXvSd9bmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_entropy(ydata):\n",
        "  \"\"\"\n",
        "  Returns the entropy of the labels in ydata (a pandas series) in bits.\n",
        "  \"\"\"\n",
        "  p = ydata.value_counts()/len(ydata)\n",
        "  return -np.sum(p*np.log2(p))"
      ],
      "metadata": {
        "id": "iofTtAFv6K0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.2"
      ],
      "metadata": {
        "id": "Q62OdE0E8Nck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_conditional_entropy(Xdata,ydata):\n",
        "  \"\"\"Return the Conditional entropy of ydata given the series (aka 'column') Xdata\n",
        "\n",
        "   - Xdata, a series (i.e. a single column) from a pandas dataframe, e.g. X['long petal']\n",
        "   - ydata, the associated labels\n",
        "\n",
        "  returns the conditional entropy H(Y|X)\n",
        "  \"\"\"\n",
        "  sum_over_x = 0\n",
        "  for v in Xdata.unique():\n",
        "    px = np.mean(Xdata==v)\n",
        "    sum_over_x += px*compute_entropy(ydata[Xdata==v])\n",
        "  return sum_over_x"
      ],
      "metadata": {
        "id": "XhpjkJMP8OdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.3"
      ],
      "metadata": {
        "id": "mkPr216p-nMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mutual_infos(Xdata,ydata):\n",
        "  \"\"\"\n",
        "  For each column in Xdata, compute the mutual information with ydata\n",
        "  (also known as information gain):\n",
        "\n",
        "     H(Y) - H(Y|X_column)\n",
        "\n",
        "  returns a pandas series with the mutual information for each column.\n",
        "  \"\"\"\n",
        "  mutual_infos = {}\n",
        "  H_Y = compute_entropy(ydata)\n",
        "  for col in Xdata.columns:\n",
        "    mutual_infos[col] = H_Y - compute_conditional_entropy(Xdata[col],ydata)\n",
        "  return pd.Series(mutual_infos)\n",
        "\n",
        "compute_mutual_infos(X,y)"
      ],
      "metadata": {
        "id": "wQcefPQr-mkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.1"
      ],
      "metadata": {
        "id": "zJhYCq9I8tKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap(x,y):\n",
        "  ws = []\n",
        "  for it in range(1000):\n",
        "    indices = np.random.choice(len(x),len(x))\n",
        "    w = fit_parameters(x[indices],y[indices])\n",
        "    ws.append(w)\n",
        "  return np.array(ws)"
      ],
      "metadata": {
        "id": "O_cA517c8Owf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.2"
      ],
      "metadata": {
        "id": "5w5kC2hg8-U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_trees(x,y):\n",
        "  \"\"\"Run the bootstrap algorithm to make splits on bootstrap subsets of the data,\n",
        "  then fit the 1-deep decision tree, using the get_split method. Return a numpy\n",
        "  array of 1000x3 values, with the first column equal to the split points, the\n",
        "  second column by the mean on the left of the split, and the third column the\n",
        "  mean of the right of the split.\"\"\"\n",
        "\n",
        "  parameters = []\n",
        "  for it in range(1000):\n",
        "    indices = np.random.choice(len(x),len(x))\n",
        "    split, left_mean, right_mean = get_split(x[indices],y[indices])\n",
        "    parameters.append([split,left_mean,right_mean])\n",
        "  return np.array(parameters)"
      ],
      "metadata": {
        "id": "LlNbQf8t89zH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}